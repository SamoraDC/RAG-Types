# naive_local.py
import os
import tempfile
import streamlit as st
from dotenv import load_dotenv

# --- Langchain Imports ---
# Core
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableLambda # RunnableLambda needed for routing
from langchain_core.runnables.history import RunnableWithMessageHistory
# Importa√ß√£o atualizada para ChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Document Loaders & Vector Stores
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS

# Ollama Specific Imports
from langchain_ollama.llms import OllamaLLM
from langchain_ollama.embeddings import OllamaEmbeddings

# Carregando vari√°veis de ambiente (opcional)
load_dotenv()

# --- Configura√ß√£o da P√°gina Streamlit ---
st.set_page_config(
    page_title="Naive-RAG com Ollama (Local)",
    page_icon="üí¨",
    layout="wide"
)

# --- Fun√ß√µes Principais ---

@st.cache_resource(show_spinner="Processando PDF e criando embeddings...")
def process_pdf(pdf_file_content, pdf_filename):
    """Carrega, divide e vetoriza o conte√∫do do PDF usando Ollama Embeddings."""
    tmp_path = None # Inicializa para garantir que existe no bloco finally
    try:
        # Salva o conte√∫do em um arquivo tempor√°rio
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(pdf_file_content)
            tmp_path = tmp_file.name

        # Carregamento do PDF
        loader = PyPDFLoader(tmp_path)
        documents = loader.load()
        if not documents:
            st.error(f"N√£o foi poss√≠vel carregar o conte√∫do do PDF: {pdf_filename}")
            return None # Retorna None se o carregamento falhar

        # Dividir texto em chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            add_start_index=True
        )
        splits = text_splitter.split_documents(documents)

        # Criar vetores com Ollama Embeddings e armazenar no FAISS
        st.info(f"Criando embeddings para '{pdf_filename}' com Ollama (nomic-embed-text)... Pode levar um tempo.")
        try:
            embeddings = OllamaEmbeddings(model="nomic-embed-text:latest")
            vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
        except Exception as e:
            st.error(f"Erro ao criar embeddings ou vector store: {e}")
            st.error("Verifique se o Ollama est√° rodando e o modelo 'nomic-embed-text:latest' est√° dispon√≠vel.")
            return None # Retorna None se a cria√ß√£o de embeddings falhar

        # Criar retriever
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 2} # N√∫mero de chunks a recuperar
        )
        st.success(f"Documento '{pdf_filename}' processado e vetorizado com sucesso!")
        return retriever

    except Exception as e:
        st.error(f"Erro geral ao processar o PDF '{pdf_filename}': {e}")
        return None # Retorna None em caso de erro geral
    finally:
        # Limpar arquivo tempor√°rio AP√ìS o uso ou em caso de erro
        if tmp_path and os.path.exists(tmp_path):
            os.unlink(tmp_path)

@st.cache_resource(show_spinner="Configurando o modelo de linguagem Ollama...")
def setup_llm():
    """Configura o modelo de linguagem OllamaLLM."""
    try:
        llm = OllamaLLM(
            model="gemma3:4b",
            max_tokens=256, # Seu modelo Ollama LLM
            temperature=0.1, # Baixa temperatura para respostas mais factuais
            # base_url="http://localhost:11434" # Descomente/ajuste se necess√°rio
        )
        # Teste r√°pido (opcional, pode ser removido se causar lentid√£o)
        # llm.invoke("Responda com 'Ok'")
        return llm
    except Exception as e:
        st.error(f"Erro ao instanciar o Ollama LLM: {e}")
        st.error("Verifique se o Ollama est√° rodando, o modelo 'phi4-mini:latest' est√° dispon√≠vel e o pacote 'langchain-ollama' est√° instalado.")
        return None

def format_docs(docs):
    """Formata os documentos recuperados em uma string √∫nica."""
    return "\n\n".join(doc.page_content for doc in docs)

@st.cache_resource(show_spinner="Montando a cadeia de RAG...")
def create_rag_chain(_llm, _retriever):
    """Cria a cadeia RAG base (sem gerenciamento de hist√≥rico expl√≠cito aqui)."""

    # 1. Prompt para contextualizar a pergunta (se houver hist√≥rico)
    contextualize_q_system_prompt = """Given a conversation (chat_history) and the user's latest question (question), 
    which may reference context from the conversation, 
    formulate an independent question that can be understood without the conversation. 
    Do not answer the question; only rephrase it if necessary; otherwise, return it as is."""
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{question}"),
        ]
    )
    contextualize_q_chain = contextualize_q_prompt | _llm | StrOutputParser()

    # 2. Prompt principal para gerar a resposta com base no contexto
    qa_system_prompt = """‚ÄãYou are a helpful assistant that answers questions based on a provided context. 
    Use ONLY the context below to formulate your response. DO NOT use prior knowledge. 
    If the information is not present in the context, clearly state "Based on the provided context, 
    I did not find the information." Be concise. And ALWAYS ANSWEAR IN PORTUGUESE PT-BR.

Contexto:
{context}
"""
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{question}"),
        ]
    )

    # 3. Define como obter a string da pergunta para o retriever
    def get_question_string(input_dict):
        if input_dict.get("chat_history"):
            # Executa a cadeia para obter a string contextualizada
            return contextualize_q_chain
        else:
            # Retorna um Runnable simples que extrai a pergunta original
            return RunnableLambda(lambda x: x['question'], name="GetOriginalQuestion")

    # 4. Monta a cadeia RAG principal com a l√≥gica corrigida
    rag_chain_base = (
        RunnablePassthrough.assign(
            # O valor de 'context' ser√° o resultado desta sub-cadeia:
            context=RunnableLambda(get_question_string, name="RouteQuestion") # Decide qual caminho seguir
                    | _retriever # Executa o retriever com a string resultante
                    | format_docs # Formata os documentos recuperados
        ).with_config(run_name="RetrieveDocs") # Adiciona nome para clareza no trace (opcional)
        | qa_prompt # Passa o dicion√°rio para o prompt final
        | _llm
        | StrOutputParser() # Garante sa√≠da como string
    )
    return rag_chain_base

# --- Gerenciamento de Mem√≥ria da Sess√£o ---
def get_session_history(session_id: str) -> ChatMessageHistory:
    """Obt√©m o hist√≥rico de chat para uma dada ID de sess√£o."""
    if session_id not in st.session_state.chat_histories:
        st.session_state.chat_histories[session_id] = ChatMessageHistory()
    return st.session_state.chat_histories[session_id]

# --- Interface Principal do Streamlit ---
def main():
    st.title("üí¨ Chat RAG com Ollama (Local)")
    st.markdown(
        """
        Carregue um documento PDF e converse sobre ele. O chatbot usar√° o conte√∫do do PDF
        e o hist√≥rico da conversa para responder suas perguntas. Modelos Ollama phi4-mini √© o usada localmente.
        **Certifique-se que o documento √© um PDF de texto selecion√°vel**
        """
    )

    # Inicializar estado da sess√£o se necess√°rio
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "chat_histories" not in st.session_state:
        st.session_state.chat_histories = {} # Dicion√°rio para guardar hist√≥ricos por sess√£o
    if "rag_chain_with_history" not in st.session_state:
        st.session_state.rag_chain_with_history = None
    if "document_loaded" not in st.session_state:
         st.session_state.document_loaded = False
    if "pdf_filename" not in st.session_state:
        st.session_state.pdf_filename = None
    if "processed_file_identifier" not in st.session_state:
        st.session_state.processed_file_identifier = None


    # --- Sidebar para Carregar Documento ---
    with st.sidebar:
        st.header("üìÑ Configura√ß√£o do Documento")
        uploaded_file = st.file_uploader("Carregar PDF", type=["pdf"], key="pdf_upload_widget")

        # L√≥gica para processar um NOVO arquivo carregado
        if uploaded_file is not None:
            new_file_identifier = f"{uploaded_file.name}-{uploaded_file.size}"
            # Processa apenas se for um arquivo diferente do √∫ltimo processado
            if st.session_state.processed_file_identifier != new_file_identifier:
                st.info(f"Novo arquivo detectado: {uploaded_file.name}")
                # Limpar estado antigo relacionado ao chat e documento anterior
                st.session_state.messages = []
                st.session_state.chat_histories = {}
                st.session_state.rag_chain_with_history = None
                st.session_state.document_loaded = False
                st.session_state.pdf_filename = None

                # Ler o conte√∫do do arquivo para passar para a fun√ß√£o cacheada
                pdf_content = uploaded_file.getvalue()
                retriever = process_pdf(pdf_content, uploaded_file.name) # Passa conte√∫do e nome

                if retriever:
                    llm = setup_llm() # Configura LLM (cacheado)
                    if llm:
                        base_rag_chain = create_rag_chain(llm, retriever) # Cria cadeia base (cacheada)
                        # Cria a cadeia com gerenciamento de hist√≥rico
                        st.session_state.rag_chain_with_history = RunnableWithMessageHistory(
                            base_rag_chain,
                            get_session_history,
                            input_messages_key="question",
                            history_messages_key="chat_history",
                            # <<< CORRE√á√ÉO: output_messages_key="answer" REMOVIDO daqui >>>
                        )
                        st.session_state.document_loaded = True
                        st.session_state.pdf_filename = uploaded_file.name
                        st.session_state.processed_file_identifier = new_file_identifier
                        # Mensagem inicial apenas ap√≥s carregar TUDO
                        st.session_state.messages.append({"role": "assistant", "content": f"Ol√°! O documento '{st.session_state.pdf_filename}' est√° pronto. Pode perguntar."})
                        st.rerun() # For√ßa o recarregamento para exibir a mensagem e limpar spinners
                    else:
                        st.error("Falha ao configurar o LLM. Chat desabilitado.")
                        st.session_state.document_loaded = False # Garante que o chat n√£o funcione
                else:
                    st.error("Falha ao processar o PDF. Chat desabilitado.")
                    st.session_state.document_loaded = False # Garante que o chat n√£o funcione
            # else: Arquivo √© o mesmo j√° processado, n√£o faz nada


        if st.session_state.document_loaded:
            st.success(f"Documento '{st.session_state.pdf_filename}' carregado.")
            if st.button("Limpar Chat e Recome√ßar"):
                 # Limpa apenas o hist√≥rico e mensagens, mant√©m o documento carregado
                 st.session_state.messages = [{"role": "assistant", "content": f"Chat reiniciado para '{st.session_state.pdf_filename}'. Fa√ßa sua pergunta."}]
                 st.session_state.chat_histories = {} # Limpa todos os hist√≥ricos da sess√£o
                 st.rerun()

        st.divider()
        st.markdown("### Como funciona:")
        st.markdown("""
        1.  **Carregue um PDF:** O conte√∫do √© extra√≠do e dividido.
        2.  **Embeddings:** O modelo local `nomic-embed-text` do Ollama cria vetores dos trechos.
        3.  **Chat:** Voc√™ faz uma pergunta.
        4.  **Contextualiza√ß√£o (com Hist√≥rico):** Se necess√°rio, o modelo `phi4-mini` reescreve sua pergunta considerando o hist√≥rico.
        5.  **Recupera√ß√£o:** A pergunta (original ou reescrita) √© usada para buscar os trechos mais relevantes no PDF.
        6.  **Gera√ß√£o:** O modelo `phi4-mini` usa o hist√≥rico, sua pergunta e os trechos recuperados para gerar a resposta final.
        """)
        st.divider()
        st.info("Pr√©-requisitos: Ollama rodando com modelos 'phi4-mini' e 'nomic-embed-text', `uv add langchain-ollama langchain-community faiss-cpu pypdf`")


    # --- L√≥gica do Chat ---
    # Exibe o chat apenas se um documento foi carregado com sucesso E a cadeia est√° pronta
    if st.session_state.document_loaded and st.session_state.rag_chain_with_history:
        # Exibir mensagens existentes
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # Campo de entrada do chat
        if prompt := st.chat_input(f"Pergunte sobre {st.session_state.pdf_filename}..."):
            # Adicionar mensagem do usu√°rio √† UI
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            # Preparar para chamar a cadeia RAG com hist√≥rico
            with st.spinner("Pensando..."):
                # ID de sess√£o simples para este exemplo
                session_id = "my_streamlit_session" # Pode ser mais elaborado se necess√°rio
                config = {"configurable": {"session_id": session_id}}

                try:
                    # Invocar a cadeia com a pergunta e a configura√ß√£o da sess√£o
                    response = st.session_state.rag_chain_with_history.invoke(
                        {"question": prompt},
                        config=config
                    )

                    # <<< CORRE√á√ÉO: L√≥gica para extrair a resposta >>>
                    if isinstance(response, str):
                        assistant_response = response
                    elif isinstance(response, dict):
                         st.warning(f"Estrutura de resposta inesperada (dict): {response}")
                         # Tenta obter de chaves comuns como fallback
                         assistant_response = response.get('output', response.get('answer', "Erro: Resposta em formato de dicion√°rio inesperado."))
                    else:
                         st.warning(f"Tipo de resposta inesperado: {type(response)}")
                         assistant_response = str(response) # Tenta converter para string como √∫ltimo recurso
                    # <<< FIM DA CORRE√á√ÉO >>>


                    # Adicionar resposta do assistente √† UI
                    # O hist√≥rico l√≥gico j√° foi atualizado pelo RunnableWithMessageHistory
                    st.session_state.messages.append({"role": "assistant", "content": assistant_response})

                    # Re-renderizar a p√°gina para mostrar a nova mensagem
                    st.rerun()

                except Exception as e:
                    st.error(f"Erro ao gerar resposta: {e}")
                    error_message = f"Ocorreu um erro ao processar sua pergunta. Verifique os logs ou se o Ollama est√° respondendo corretamente."
                    st.session_state.messages.append({"role": "assistant", "content": error_message})
                    st.rerun()

    # Mensagens informativas se o chat n√£o estiver pronto
    elif 'document_loaded' in st.session_state and not st.session_state.document_loaded:
        st.warning("Houve um erro ao carregar o documento ou configurar o LLM. Verifique as mensagens de erro.")
    elif not st.session_state.get('processed_file_identifier'): # Se nenhum arquivo foi processado ainda
         st.info("‚¨ÖÔ∏è Carregue um documento PDF na barra lateral para come√ßar.")


if __name__ == "__main__":
    # Verifica√ß√µes Iniciais de importa√ß√£o
    try:
        from langchain_ollama.llms import OllamaLLM
        from langchain_ollama.embeddings import OllamaEmbeddings
        from langchain_community.chat_message_histories import ChatMessageHistory
        from langchain_community.vectorstores import FAISS
        # Adicione outras importa√ß√µes cr√≠ticas se necess√°rio
    except ImportError as e:
        st.error(f"Erro Cr√≠tico de Importa√ß√£o: {e}")
        st.error("Certifique-se de ter instalado todos os pacotes necess√°rios:")
        st.code("pip install streamlit langchain langchain-core langchain-community langchain-ollama faiss-cpu pypdf python-dotenv")
        st.stop() # Impede a execu√ß√£o do app se faltar pacote essencial

    main()