import os
import tempfile
import uuid
import streamlit as st
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain.retrievers import EnsembleRetriever
from langchain.schema import Document
from datetime import datetime
import time

# Carregando vari√°veis de ambiente
load_dotenv()

# Configura√ß√£o da p√°gina Streamlit
st.set_page_config(
    page_title="H√≠brido RAG - Sistema de Consulta Avan√ßado",
    page_icon="üîç",
    layout="wide"
)

# Fun√ß√£o para inicializar a sess√£o
def init_session_state():
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "document_loaded" not in st.session_state:
        st.session_state.document_loaded = False
    if "conversation_id" not in st.session_state:
        st.session_state.conversation_id = str(uuid.uuid4())
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "chat_log" not in st.session_state:
        st.session_state.chat_log = {}

# Inicializar sess√£o
init_session_state()

# Fun√ß√£o para processamento de PDF com RAG h√≠brido
def process_pdf(pdf_file):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(pdf_file.getvalue())
        tmp_path = tmp_file.name
    
    try:
        # Carregamento do PDF
        loader = PyPDFLoader(tmp_path)
        documents = loader.load()
        
        # Dividir texto em chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(documents)
        
        # RETRIEVER 1: Criar vetores e armazenar no FAISS (busca sem√¢ntica)
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
        vector_retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4}
        )
        
        # RETRIEVER 2: Configurar BM25 para busca l√©xica (baseada em keywords)
        texts = [doc.page_content for doc in splits]
        bm25_retriever = BM25Retriever.from_texts(texts=texts)
        bm25_retriever.k = 4
        
        # Converter documentos do BM25 para o formato adequado
        def format_docs(docs):
            return [
                Document(
                    page_content=doc.page_content,
                    metadata={"source": f"chunk_{i}", "score": doc.metadata.get("score", 0)}
                )
                for i, doc in enumerate(docs)
            ]
        
        # Wrapper para o BM25 para garantir compatibilidade
        bm25_compatible = RunnableLambda(lambda x: format_docs(bm25_retriever.get_relevant_documents(x)))
        
        return {
            "vector_retriever": vector_retriever,
            "bm25_retriever": bm25_compatible,
            "splits": splits
        }
    
    finally:
        # Limpar arquivo tempor√°rio
        os.unlink(tmp_path)

# Fun√ß√£o para aplicar Reciprocal Rank Fusion
def reciprocal_rank_fusion(results_lists, k=60):
    """
    Implementa o algoritmo Reciprocal Rank Fusion para combinar resultados de m√∫ltiplos retrievers.
    
    Args:
        results_lists: Lista de listas de documentos de diferentes retrievers
        k: Constante de suaviza√ß√£o (60 √© valor padr√£o na literatura)
        
    Returns:
        Lista de documentos ordenados por pontua√ß√£o RRF
    """
    # Dicion√°rio para armazenar documentos e suas pontua√ß√µes
    doc_scores = {}
    
    # Para cada lista de resultados
    for i, results in enumerate(results_lists):
        # Para cada documento na lista
        for rank, doc in enumerate(results):
            # Calcula a pontua√ß√£o RRF para este documento nesta lista
            # Rank come√ßa em 0, ent√£o adicionamos 1
            rrf_score = 1 / (k + rank + 1)
            
            # Identifica o documento por conte√∫do (como chave para o dicion√°rio)
            doc_key = doc.page_content
            
            # Adiciona a pontua√ß√£o RRF para este documento
            if doc_key not in doc_scores:
                doc_scores[doc_key] = {"doc": doc, "score": 0}
            doc_scores[doc_key]["score"] += rrf_score
    
    # Ordenar documentos por pontua√ß√£o RRF em ordem decrescente
    sorted_docs = sorted(
        doc_scores.values(),
        key=lambda x: x["score"],
        reverse=True
    )
    
    # Retornar apenas os documentos, n√£o as pontua√ß√µes
    return [item["doc"] for item in sorted_docs]

# Fun√ß√£o para configurar modelo LLM
def setup_llm():
    # Verificar se a API KEY est√° dispon√≠vel
    if not os.getenv("GROQ_API_KEY"):
        st.error("API KEY da Groq n√£o encontrada. Configure GROQ_API_KEY no arquivo .env")
        return None
    
    # Usando Groq como LLM
    llm = ChatGroq(
        model_name="llama3-8b-8192",
        temperature=0.2,
        groq_api_key=os.getenv("GROQ_API_KEY")
    )
    
    return llm

# Fun√ß√£o para criar o pipeline de RAG H√≠brido
def create_hybrid_rag_pipeline(vector_retriever, bm25_retriever, llm):
    # Fun√ß√£o de RAG h√≠brida com Reciprocal Rank Fusion
    def retrieve_and_fuse(query):
        # Obter resultados dos dois retrievers
        vector_docs = vector_retriever.get_relevant_documents(query)
        bm25_docs = bm25_retriever.invoke(query)
        
        # Aplicar Reciprocal Rank Fusion
        fused_docs = reciprocal_rank_fusion([vector_docs, bm25_docs])
        
        # Limitar a 6 documentos para n√£o sobrecarregar o contexto
        return fused_docs[:6]
    
    # Template de prompt aprimorado para RAG com hist√≥rico
    template = """
    Voc√™ √© um assistente especializado em an√°lise de documentos com mem√≥ria de conversa√ß√£o.
    
    Use apenas o contexto abaixo e o hist√≥rico de conversas para responder √† pergunta.
    Se a informa√ß√£o n√£o estiver no contexto, responda que n√£o consegue encontrar a informa√ß√£o solicitada.
    
    Hist√≥rico de conversas:
    {chat_history}
    
    Contexto:
    {context}
    
    Pergunta: {question}
    
    Resposta:
    """
    
    # Criar prompt
    prompt = ChatPromptTemplate.from_template(template)
    
    # Fun√ß√£o para formatar o hist√≥rico de chat
    def format_chat_history(chat_history):
        if not chat_history:
            return "Nenhum hist√≥rico de conversas anterior."
        formatted_history = ""
        for i, (q, a) in enumerate(chat_history):
            formatted_history += f"Pergunta {i+1}: {q}\nResposta {i+1}: {a}\n\n"
        return formatted_history
    
    # Fun√ß√£o para formatar o contexto
    def format_context(docs):
        return "\n\n".join([f"Documento {i+1}:\n{doc.page_content}" for i, doc in enumerate(docs)])
    
    # Montar pipeline RAG com hist√≥rico
    rag_chain = (
        {
            "context": lambda x: format_context(retrieve_and_fuse(x["question"])),
            "question": lambda x: x["question"],
            "chat_history": lambda x: format_chat_history(x["chat_history"])
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain

# Interface principal estilizada como chat
def main():
    st.title("üîç Sistema Avan√ßado de H√≠brido RAG")
    
    # Sidebar para carregar documento
    with st.sidebar:
        st.header("Configura√ß√µes")
        pdf_file = st.file_uploader("Carregar PDF", type=["pdf"])
        
        if pdf_file:
            # Mostrar nome do arquivo
            st.success(f"Documento carregado: {pdf_file.name}")
            
            # Processar documento
            with st.spinner("Processando documento com an√°lise h√≠brida..."):
                retrievers = process_pdf(pdf_file)
                llm = setup_llm()
                
                if llm is None:
                    return
                
                rag_chain = create_hybrid_rag_pipeline(
                    retrievers["vector_retriever"],
                    retrievers["bm25_retriever"],
                    llm
                )
                
                # Guardar componentes na sess√£o
                st.session_state.rag_chain = rag_chain
                st.session_state.document_loaded = True
                st.session_state.document_name = pdf_file.name
        
        # Sistema de hist√≥rico de conversas
        st.header("Hist√≥rico de Conversas")
        
        if "chat_log" in st.session_state and st.session_state.chat_log:
            # Mostrar conversas anteriores (at√© 5)
            conversations = list(st.session_state.chat_log.items())
            conversations.sort(key=lambda x: x[1]['timestamp'], reverse=True)
            
            for i, (conv_id, conv_data) in enumerate(conversations[:5]):
                if st.button(f"{conv_data['document'][:20]}... ({conv_data['timestamp'].strftime('%d/%m %H:%M')})", key=f"conv_{conv_id}"):
                    # Carregar hist√≥rico selecionado
                    st.session_state.conversation_id = conv_id
                    st.session_state.messages = conv_data['messages']
                    st.session_state.chat_history = conv_data['chat_history']
                    st.experimental_rerun()
        
        # Bot√£o para nova conversa
        if st.button("Nova Conversa", key="new_conversation"):
            st.session_state.conversation_id = str(uuid.uuid4())
            st.session_state.messages = []
            st.session_state.chat_history = []
            st.experimental_rerun()
        
        # Exibir explica√ß√£o sobre o H√≠brido RAG
        st.markdown("### Como funciona o H√≠brido RAG")
        st.markdown("""
        1. **Recupera√ß√£o Vetorial**: Busca por similaridade sem√¢ntica
        2. **Recupera√ß√£o BM25**: Busca baseada em palavras-chave
        3. **Fus√£o de Rankings**: Combina resultados usando RRF
        4. **Mem√≥ria de Conversa√ß√£o**: Mant√©m contexto entre perguntas
        """)

    # Container para mensagens de chat
    chat_container = st.container()
    
    # Formul√°rio para envio de mensagem
    with st.form(key="message_form", clear_on_submit=True):
        col1, col2 = st.columns([5, 1])
        
        with col1:
            user_input = st.text_input(
                "Digite sua pergunta:",
                placeholder="O que voc√™ gostaria de saber sobre o documento?",
                label_visibility="collapsed"
            )
        
        with col2:
            submit_button = st.form_submit_button("Enviar")
    
    # Processar mensagem quando enviada
    if submit_button and user_input and st.session_state.document_loaded:
        # Adicionar mensagem do usu√°rio ao hist√≥rico de exibi√ß√£o
        st.session_state.messages.append({"role": "user", "content": user_input})
        
        # Mostrar indicador de "digitando"
        with chat_container:
            messages_placeholder = st.empty()
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.write(message["content"])
            
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                message_placeholder.markdown("Pensando...")
        
        # Executar pipeline RAG
        with st.spinner(""):
            try:
                response = st.session_state.rag_chain.invoke({
                    "question": user_input,
                    "chat_history": st.session_state.chat_history
                })
                
                # Adicionar √† mem√≥ria de conversa (mantenha apenas as √∫ltimas 5 intera√ß√µes)
                st.session_state.chat_history.append((user_input, response))
                if len(st.session_state.chat_history) > 5:
                    st.session_state.chat_history.pop(0)
                
                # Adicionar mensagem do assistente ao hist√≥rico de exibi√ß√£o
                st.session_state.messages.append({"role": "assistant", "content": response})
                
                # Salvar no log de conversas
                st.session_state.chat_log[st.session_state.conversation_id] = {
                    "document": st.session_state.document_name,
                    "timestamp": datetime.now(),
                    "messages": st.session_state.messages,
                    "chat_history": st.session_state.chat_history
                }
                
                # Atualizar a mensagem de "digitando" com a resposta real
                message_placeholder.markdown(response)
                
            except Exception as e:
                st.error(f"Erro ao processar a resposta: {str(e)}")
    
    elif submit_button and not st.session_state.document_loaded:
        st.warning("Por favor, carregue um documento PDF primeiro.")
    
    # Exibir hist√≥rico de mensagens no container de chat
    with chat_container:
        if st.session_state.messages:
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.write(message["content"])
        else:
            # Mensagem inicial quando n√£o h√° hist√≥rico
            st.markdown("""
            ### üëã Bem-vindo ao Sistema H√≠brido RAG!
            
            Para come√ßar:
            1. Carregue um documento PDF no painel lateral
            2. Fa√ßa perguntas sobre o conte√∫do do documento
            3. O sistema combinar√° busca sem√¢ntica e l√©xica para encontrar as informa√ß√µes mais relevantes
            
            Este sistema mant√©m mem√≥ria das √∫ltimas 5 intera√ß√µes para fornecer respostas contextualizadas.
            """)

# Executar aplica√ß√£o
if __name__ == "__main__":
    main()